version: '3.8'

services:
  redis:
    image: redis:alpine
    container_name: redis
    volumes:
      - redis_data:/data
    command: [ "redis-server", "--save", "60 1", "--appendonly", "yes" ]
    ports:
      - "6379:6379"

  gpt-4-mini:
    build: ./inference_service
    container_name: inference_service_gpt4mini
    ports:
      - "8002:8000"
    environment:
      OPENAI_API_KEY: ${OPENAI_API_KEY}
      MODEL_NAME: "gpt-4o-mini"

  gpt-4o:
    build: ./inference_service
    container_name: inference_service_gpt4o
    ports:
      - "8003:8000"
    environment:
      OPENAI_API_KEY: ${OPENAI_API_KEY}
      MODEL_NAME: "gpt-4o"

  llama-405b:
    build: ./inference_service
    container_name: inference_service_llama_3_1_405b
    ports:
      - "8004:8000"
    environment:
      NEBIUS_API_KEY: ${NEBIUS_API_KEY}
      CLIENT_NAME: "nebius"
      MODEL_NAME: "meta-llama/Meta-Llama-3.1-405B-Instruct"

  llama-70b:
    build: ./inference_service
    container_name: inference_service_llama_3_1_70b
    ports:
      - "8005:8000"
    environment:
      NEBIUS_API_KEY: ${NEBIUS_API_KEY}
      CLIENT_NAME: "nebius"
      MODEL_NAME: "meta-llama/Meta-Llama-3.1-70B-Instruct"

  llama-8b:
    build: ./inference_service
    container_name: inference_service_llama_3_1_8b
    ports:
      - "8006:8000"
    environment:
      NEBIUS_API_KEY: ${NEBIUS_API_KEY}
      CLIENT_NAME: "nebius"
      MODEL_NAME: "meta-llama/Meta-Llama-3.1-8B-Instruct"

#  ollama:
#    image: ollama/ollama
#    container_name: ollama_service
#    ports:
#      - "11434:11434"
#    pull_policy: always
#    tty: true
#    restart: always
#    volumes:
#      - ollama_data:/root/.ollama  # Persistent storage for models
#    entrypoint: ["/bin/sh", "-c", "ollama serve & sleep 5 && ollama pull llama3 && wait"]
#    entrypoint: ["/bin/sh", "-c", "ollama serve & sleep 5 && ollama pull mistral && wait"]
#    entrypoint: ["/bin/sh", "-c", "ollama serve & sleep 5 && ollama pull llama3 && ollama pull mistral && ollama pull gemma && ollama pull phi3 && wait"]


  gateway_service:
    build: ./gateway_service
    container_name: gateway_service
    ports:
      - "8001:8001"
    depends_on:
      - redis
      - gpt-4-mini
      - gpt-4o
      - postgres
#      - ollama
    environment:
      ADMIN_KEY: ${ADMIN_KEY}
      MODEL_NAME: "meta-llama/Meta-Llama-3.1-8B-Instruct"
#      OLLAMA_API_URL: "http://ollama:11434"

  postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: user
      POSTGRES_PASSWORD: password
      POSTGRES_DB: dbname
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./postgres_init:/docker-entrypoint-initdb.d

volumes:
  postgres_data:
    driver: local
  redis_data:
    driver: local
#  o`llama_data:
#    driver: local